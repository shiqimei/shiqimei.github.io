<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Claude Code for Consulting & Data Analysis: Treating Large Datasets as Codebases - Shiqi Mei</title>
  <link rel="icon" type="image/png" href="../avatar.png">
  <link rel="stylesheet" href="../css/style.css">
</head>
<body>
  <div class="container">
    <a href="/" class="back-link">&larr; back</a>

    <article>
      <header class="post-header">
        <div class="post-header-row">
          <h1 class="post-title">Claude Code for Consulting & Data Analysis: Treating Large Datasets as Codebases</h1>
          <div class="lang-toggle" id="langToggle">
            <button data-lang="en" class="active">EN</button>
            <button data-lang="zh">CN</button>
          </div>
        </div>
        <p class="post-meta">Jan 18, 2026</p>
      </header>

      <div class="post-content">
        <img src="../images/claude-code-data-analysis-pipeline.svg" class="post-hero">

<div class="lang-en">

<p>Consulting with large datasets. 100K-1M records. Extract themes. Dedupe. Classify. Generate reports. No hallucinations.</p>
<p>This post: using Claude Code for enterprise data analysis. Real project. 25K+ news articles. Biweekly media monitoring reports.</p>
<h2>The Problem</h2>
<p>Given a massive dataset, how do you:</p>
<ol>
<li>Identify top themes</li>
<li>Deduplicate similar content</li>
<li>Classify by business relevance</li>
<li>Generate summaries without hallucinations</li>
<li>Output formatted reports (Excel, PDF)</li>
</ol>
<p>Three approaches. Two fail. One works.</p>
<h2>Approach 1: Embeddings + Clustering</h2>
<p>Traditional ML:</p>
<ol>
<li>Generate embeddings (Qwen3-Embedding-0.6B)</li>
<li>Density-based clustering (HDBSCAN)</li>
<li>Recursive sub-clustering</li>
<li>Small models for summaries</li>
</ol>
</div>

<div class="lang-zh">

<p>大规模数据集做咨询。10万-100万条记录。提取主题。去重。分类。生成报告。不能有幻觉。</p>
<p>本文：用 Claude Code 做企业数据分析。真实项目。2.5万多篇新闻。双周舆情监测报告。</p>
<h2>问题</h2>
<p>给定海量数据集，如何：</p>
<ol>
<li>识别顶级主题</li>
<li>去重相似内容</li>
<li>按业务相关性分类</li>
<li>生成摘要不产生幻觉</li>
<li>产出格式化报告（Excel、PDF）</li>
</ol>
<p>三种方法。两种不行。一种有效。</p>
<h2>方法一：嵌入向量 + 聚类</h2>
<p>传统机器学习：</p>
<ol>
<li>生成嵌入向量（Qwen3-Embedding-0.6B）</li>
<li>基于密度的聚类（HDBSCAN）</li>
<li>递归子聚类</li>
<li>小模型生成摘要</li>
</ol>
</div>

<pre><code class="language-text">Raw JSON Data (25K articles)
    ↓ [Qwen3-Embedding-0.6B, 1024 dims]
Embedding Vectors
    ↓ [HDBSCAN, conservative params]
L1 Clusters (30-150 clusters)
    ↓ [Recursive if &gt; 10K tokens]
L2/L3 Sub-clusters
    ↓ [Qwen3-30B summaries]
Cluster Summaries</code></pre><div class="lang-en">

<p><strong>Why it fails:</strong> News data is sparse. Not dense. HDBSCAN needs natural clusters. News articles? Diverse topics. Weak semantic connections.</p>
<p>Results:</p>
<ul>
<li>50%+ unclustered</li>
<li>Aggressive params → micro-clusters</li>
<li>Best case: 30%+ unclassified</li>
</ul>
<p>Sparse data. Clustering fails.</p>
<h2>Approach 2: Large Context LLMs</h2>
<p>Modern context windows:</p>
<ul>
<li>Claude Opus 4.5: 200K tokens</li>
<li>Gemini 3 Pro: 1M tokens</li>
</ul>
<p>Strategies:</p>
<ul>
<li><strong>Sampling</strong>: 10% of data</li>
<li><strong>Chunking</strong>: Split and process separately</li>
</ul>
<p><strong>Why it fails:</strong> Hallucination compounds with context length.</p>
<p>More tokens → weaker attention to facts. Errors accumulate:</p>
<ul>
<li>Dates shift</li>
<li>Numbers drift</li>
<li>Sources confused</li>
<li>Details fabricated</li>
</ul>
<p>Large context: necessary but insufficient. Need verification.</p>
<h2>Approach 3: Codebase File Editing</h2>
<p>Key insight: <strong>coding agents are optimized for file operations.</strong></p>
<p>Claude Code tools:</p>
<ul>
<li><code>Read</code>: up to 2000 lines</li>
<li><code>Edit</code>: surgical changes</li>
<li><code>Write</code>: file creation</li>
<li><code>Bash</code>: unix commands</li>
</ul>
<p>Reframe: dataset problem → codebase problem.</p>
</div>

<div class="lang-zh">

<p><strong>失败原因：</strong> 新闻数据是稀疏的。不是稠密的。HDBSCAN 需要自然聚簇。新闻文章？主题多样。语义关联弱。</p>
<p>结果：</p>
<ul>
<li>50%以上无法聚类</li>
<li>激进参数 → 微型簇</li>
<li>最优情况：30%以上无法分类</li>
</ul>
<p>数据稀疏。聚类失败。</p>
<h2>方法二：大上下文 LLM</h2>
<p>现代上下文窗口：</p>
<ul>
<li>Claude Opus 4.5：20万 tokens</li>
<li>Gemini 3 Pro：100万 tokens</li>
</ul>
<p>策略：</p>
<ul>
<li><strong>采样</strong>：取10%数据</li>
<li><strong>分块</strong>：拆分后分别处理</li>
</ul>
<p><strong>失败原因：</strong> 幻觉随上下文长度累积。</p>
<p>更多 tokens → 事实注意力减弱。错误累积：</p>
<ul>
<li>日期偏移</li>
<li>数字漂移</li>
<li>来源混淆</li>
<li>细节虚构</li>
</ul>
<p>大上下文：必要但不充分。需要验证。</p>
<h2>方法三：当作代码库文件编辑</h2>
<p>关键洞察：<strong>编程智能体针对文件操作优化。</strong></p>
<p>Claude Code 工具：</p>
<ul>
<li><code>Read</code>：最多2000行</li>
<li><code>Edit</code>：精确修改</li>
<li><code>Write</code>：文件创建</li>
<li><code>Bash</code>：unix 命令</li>
</ul>
<p>重新定义：数据集问题 → 代码库问题。</p>
</div>

<pre><code class="language-text">Excel/JSON Data
    ↓ [Preprocessing scripts]
CSV Files (row-based, compact format)
    ↓ [Split into chunks]
200 lines per file × N files
    ↓ [Claude Code operates on files]
Tagged/Labeled/Merged CSV Files</code></pre><div class="lang-en">

<h3>The Conversion</h3>
<ol>
<li><strong>Export to CSV</strong>: Row-based format</li>
<li><strong>Preprocess</strong>: Remove whitespace, compact</li>
<li><strong>Chunk</strong>: ~200 records per file (fits 2000 lines)</li>
<li><strong>Treat as code</strong>: CSV files = source code</li>
</ol>
<p>200K records → ~1000 CSV files. Normal codebase. Claude Code handles it.</p>
<h3>Labeling at Scale</h3>
</div>

<div class="lang-zh">

<h3>转换方法</h3>
<ol>
<li><strong>导出 CSV</strong>：行式格式</li>
<li><strong>预处理</strong>：去空白，压缩</li>
<li><strong>分块</strong>：每文件约200条（适配2000行）</li>
<li><strong>当作代码</strong>：CSV 文件 = 源代码</li>
</ol>
<p>20万条 → 约1000个 CSV。正常代码库。Claude Code 处理。</p>
<h3>大规模标注</h3>
</div>

<pre><code class="language-diff">- Column A|Column B|...
+ Tag1,Tag2,Tag3|Column A|Column B|...</code></pre><div class="lang-en">

<p>Claude Code edits CSV files. Adds tags to rows. Just file editing.</p>
<h2>Parallel Task Agents</h2>
<p>10 agents. Simultaneously. One agent per CSV chunk:</p>
</div>

<div class="lang-zh">

<p>Claude Code 编辑 CSV。为行添加标签。就是文件编辑。</p>
<h2>并行 Task 智能体</h2>
<p>10个智能体。同时运行。每个 CSV 块一个智能体：</p>
</div>

<pre><code class="language-text">Running 10 Task agents... (ctrl+o to expand)
   ├─ Tag batch 1 summaries · 12 tool uses
   │  ⎿  Processing summary_01.csv...
   ├─ Tag batch 2 summaries · 8 tool uses
   │  ⎿  Processing summary_02.csv...
   ├─ Tag batch 3 summaries · 15 tool uses
   │  ⎿  Processing summary_03.csv...
   ├─ Tag batch 4 summaries · 11 tool uses
   │  ⎿  Processing summary_04.csv...
   ├─ Tag batch 5 summaries · 9 tool uses
   │  ⎿  Processing summary_05.csv...
   ├─ Tag batch 6 summaries · 14 tool uses
   │  ⎿  Processing summary_06.csv...
   ├─ Tag batch 7 summaries · 7 tool uses
   │  ⎿  Processing summary_07.csv...
   ├─ Tag batch 8 summaries · 13 tool uses
   │  ⎿  Processing summary_08.csv...
   ├─ Tag batch 9 summaries · 10 tool uses
   │  ⎿  Processing summary_09.csv...
   └─ Tag batch 10 summaries · 6 tool uses
      ⎿  Processing summary_10.csv...</code></pre><div class="lang-en">

<p>10x parallelism. Task tool: most powerful feature for consulting.</p>
<h2>The IR Pattern</h2>
<p>IR = Intermediate Representation. Key pattern for reliable reports.</p>
</div>

<div class="lang-zh">

<p>10倍并行。Task 工具：咨询最强功能。</p>
<h2>IR 模式</h2>
<p>IR = 中间表示。可靠报告的关键模式。</p>
</div>

<pre><code class="language-text">Raw CSV Data
    ↓ [Labeling, merging, re-ranking, scoring]
PostgreSQL Database (with tags)
    ↓ [Query and filter]
IR (Intermediate Representation - JSON)
    ↓ [Normalization via Task agents]
Normalized IR (deduplicated, translated)
    ↓ [Report generation scripts]
Final Reports (Markdown → XLSX/PDF)</code></pre><div class="lang-en">

<h3>Why IR</h3>
<p>Checkpoint between processing and generation:</p>
<ol>
<li><strong>Auditable</strong>: Inspect before output</li>
<li><strong>Iterative</strong>: Regenerate without reprocessing</li>
<li><strong>Verifiable</strong>: Facts trace to IR</li>
</ol>
<h3>Two-Phase Generation</h3>
<p><strong>Phase 1: Raw IR</strong></p>
<ul>
<li>Query with tag/source filters</li>
<li>Output candidate JSON</li>
</ul>
<p><strong>Phase 2: Normalized IR</strong></p>
<ul>
<li>Merge duplicates</li>
<li>Add translations</li>
<li>Normalize dates (YYYY-MM-DD)</li>
<li>Validate metadata</li>
</ul>
<p>Separation prevents error propagation.</p>
<h2>Report Validation</h2>
<p>Critical: <strong>every fact needs a source reference.</strong></p>
</div>

<div class="lang-zh">

<h3>为什么用 IR</h3>
<p>处理和生成之间的检查点：</p>
<ol>
<li><strong>可审计</strong>：输出前检查</li>
<li><strong>可迭代</strong>：无需重新处理即可重新生成</li>
<li><strong>可验证</strong>：事实追溯到 IR</li>
</ol>
<h3>两阶段生成</h3>
<p><strong>第一阶段：原始 IR</strong></p>
<ul>
<li>用标签/来源过滤器查询</li>
<li>输出候选 JSON</li>
</ul>
<p><strong>第二阶段：规范化 IR</strong></p>
<ul>
<li>合并重复</li>
<li>添加翻译</li>
<li>规范化日期（YYYY-MM-DD）</li>
<li>验证元数据</li>
</ul>
<p>分离防止错误传播。</p>
<h2>报告验证</h2>
<p>关键：<strong>每个事实都需要来源引用。</strong></p>
</div>

<pre><code class="language-text">Report Markdown
    ↓ [Extract facts script]
.validation.json (dates, numbers, claims)
    ↓ [Parallel Task agents search sources]
Source references added
    ↓ [Verification script]
Check for NOT_FOUND entries</code></pre><div class="lang-en">

<h3>report-validator Skill</h3>
<p>3 steps:</p>
<ol>
<li><strong>Extract</strong>: Parse markdown → <code>.validation.json</code></li>
<li><strong>Search</strong>: Task agents find sources in parallel</li>
<li><strong>Check</strong>: Verify no <code>&quot;NOT_FOUND&quot;</code></li>
</ol>
<h3>Smart Matching</h3>
<p>Handles format variations:</p>
<ul>
<li><strong>Dates</strong>: &quot;2026-01-07&quot; = &quot;January 7&quot; = &quot;1月7日&quot;</li>
<li><strong>Numbers</strong>: &quot;1.8B&quot; = &quot;18亿&quot; = &quot;1.8 billion&quot;</li>
</ul>
<p>Core values must exist in source. Format doesn&#39;t matter.</p>
<h3>Zero Tolerance</h3>
<p>Any missing source → validation fails. Forces:</p>
<ul>
<li>Accurate transcription</li>
<li>No invented details</li>
<li>Traceable claims</li>
</ul>
<h2>Agent Skills</h2>
<p>Reusable instruction sets. Encapsulated domain expertise.</p>
</div>

<div class="lang-zh">

<h3>report-validator 技能</h3>
<p>3步：</p>
<ol>
<li><strong>提取</strong>：解析 markdown → <code>.validation.json</code></li>
<li><strong>搜索</strong>：Task 智能体并行查找来源</li>
<li><strong>检查</strong>：验证无 <code>&quot;NOT_FOUND&quot;</code></li>
</ol>
<h3>智能匹配</h3>
<p>处理格式变体：</p>
<ul>
<li><strong>日期</strong>：&quot;2026-01-07&quot; = &quot;January 7&quot; = &quot;1月7日&quot;</li>
<li><strong>数字</strong>：&quot;1.8B&quot; = &quot;18亿&quot; = &quot;1.8 billion&quot;</li>
</ul>
<p>核心值必须存在于来源。格式无所谓。</p>
<h3>零容忍</h3>
<p>任何缺失来源 → 验证失败。强制：</p>
<ul>
<li>准确转录</li>
<li>无虚构细节</li>
<li>可追溯声明</li>
</ul>
<h2>Agent 技能</h2>
<p>可复用指令集。封装领域专业知识。</p>
</div>

<pre><code class="language-text">.claude/skills/
├── generate-report/     # Two-phase IR → Report workflow
├── tag-summaries/       # Multi-tag classification
├── report-validator/    # Fact verification
├── xlsx/                # Excel generation with formulas
└── analyze-cluster/     # Cluster metadata extraction</code></pre><div class="lang-en">

<h3>Key Skills for Data Analysis</h3>
<p><strong>generate-report</strong>: Orchestrates the full pipeline</p>
<ul>
<li>Phase 0: Database deduplication</li>
<li>Phase 1: Raw IR generation</li>
<li>Phase 2: IR normalization</li>
<li>Phase 3: Markdown generation</li>
<li>Phase 4: Excel export</li>
</ul>
<p><strong>tag-summaries</strong>: Multi-label classification</p>
<ul>
<li>Primary tags: market policy, opportunity, company news, competitor, industry</li>
<li>Secondary tags: negative sentiment categories</li>
<li>Social media tags: platform-specific variants</li>
</ul>
<p><strong>xlsx</strong>: Enterprise Excel generation</p>
<ul>
<li>Formula preservation (never hardcode calculated values)</li>
<li>Error checking (zero #REF!, #DIV/0!, etc.)</li>
<li>Professional styling (column widths, alignment, wrapping)</li>
</ul>
<h2>The Complete Architecture</h2>
</div>

<div class="lang-zh">

<h3>数据分析的关键技能</h3>
<p><strong>generate-report</strong>：编排完整流程</p>
<ul>
<li>第0阶段：数据库去重</li>
<li>第1阶段：原始 IR 生成</li>
<li>第2阶段：IR 规范化</li>
<li>第3阶段：Markdown 生成</li>
<li>第4阶段：Excel 导出</li>
</ul>
<p><strong>tag-summaries</strong>：多标签分类</p>
<ul>
<li>主标签：市场政策、市场机会、公司新闻、竞品、行业</li>
<li>次标签：负面情绪分类</li>
<li>社媒标签：平台特定变体</li>
</ul>
<p><strong>xlsx</strong>：企业级 Excel 生成</p>
<ul>
<li>公式保留（永不硬编码计算值）</li>
<li>错误检查（零 #REF!、#DIV/0! 等）</li>
<li>专业样式（列宽、对齐、自动换行）</li>
</ul>
<h2>完整架构</h2>
</div>

<img src="../images/claude-code-data-analysis-architecture.svg" style="width:100%;max-width:700px;margin:1.5rem auto;display:block;">

<div class="lang-en">

<h2>Key Takeaways</h2>
<ol>
<li><p><strong>Reframe the problem</strong>: Data analysis → codebase file editing. Claude Code is optimized for this.</p>
</li>
<li><p><strong>Chunk strategically</strong>: 200 records per file, 2000 lines max. Fits Claude Code&#39;s tools perfectly.</p>
</li>
<li><p><strong>Parallelize with Task agents</strong>: 10x throughput for labeling, classification, validation.</p>
</li>
<li><p><strong>Use IR as checkpoint</strong>: Separate data processing from report generation. Enables iteration and auditing.</p>
</li>
<li><p><strong>Validate everything</strong>: The report-validator pattern catches hallucinations before delivery.</p>
</li>
<li><p><strong>Encapsulate in skills</strong>: Domain expertise becomes reusable, maintainable instruction sets.</p>
</li>
</ol>
<p>The result: enterprise-quality reports from 25K+ articles, with every fact traceable to source data.</p>
<p>Claude Code isn&#39;t just for writing code. It&#39;s a general-purpose agent for any task that can be expressed as file operations. Data analysis fits perfectly.</p>
</div>

<div class="lang-zh">

<h2>关键要点</h2>
<ol>
<li><p><strong>重新定义问题</strong>：数据分析 → 代码库文件编辑。Claude Code 针对此做了优化。</p>
</li>
<li><p><strong>策略性分块</strong>：每文件200条记录，最多2000行。适配 Claude Code 的工具。</p>
</li>
<li><p><strong>用 Task 智能体并行化</strong>：标注、分类、验证获得10倍吞吐量。</p>
</li>
<li><p><strong>使用 IR 作为检查点</strong>：分离数据处理和报告生成。支持迭代和审计。</p>
</li>
<li><p><strong>验证一切</strong>：report-validator 模式在交付前捕获幻觉。</p>
</li>
<li><p><strong>封装为技能</strong>：领域专业知识变成可复用、可维护的指令集。</p>
</li>
</ol>
<p>结果：从2.5万多篇文章生成企业级报告，每个事实都可追溯到源数据。</p>
<p>Claude Code 不只是写代码。它是通用智能体，适用于任何可以表达为文件操作的任务。数据分析完美契合。</p>
</div>

      </div>
    </article>

    <footer class="site-footer">
      <p>&copy; 2026 Shiqi Mei</p>
    </footer>
  </div>
  <script type="module" src="../js/highlight.js"></script>
  <script src="../js/lang-toggle.js"></script>
</body>
</html>
